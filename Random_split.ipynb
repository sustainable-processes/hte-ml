{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random split using various models and datarepresentations\n",
    "\n",
    "### Models: ANN, RF, GP, Linear Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required packages \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# for RF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# for GP\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF,Matern, ConstantKernel as C\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel\n",
    "\n",
    "# for adaboost\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# for SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# for the NN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "#install package for excel support\n",
    "#!pip install xlrd --user\n",
    "import xlrd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "\n",
    "from rdkit import rdBase\n",
    "rdBase.rdkitVersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the used set of descriptors\n",
    "\n",
    "input = fingerprints_input\n",
    "#input = np.append(input,data_2_sterimol,axis=1)\n",
    "#input = np.append(input,dft_data,axis=1)\n",
    "#input = np.append(input,fp_pca_data,axis=1)\n",
    "#input = np.append(input,dataset_one_hot,axis=1)\n",
    "#input = np.append(input,NBO_data,axis=1)\n",
    "#input = np.append(input,chelpg_data,axis=1)\n",
    "#input = np.append(input,vol_bur,axis=1)\n",
    "#input = np.append(input,n_proton,axis=1)\n",
    "# input = np.append(input,data_only_R_sterimol,axis=1)\n",
    "# input = np.append(input,chelpg_data,axis=1)\n",
    "\n",
    "#input = shuffle(input)\n",
    "print('total_input shape:',input.shape)\n",
    "\n",
    "output = dataset['Yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data partitioning\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(input, output, test_size=0.2, random_state=1)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "input_dimension = (train_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(train_features, train_labels)\n",
    "linear_test_predictions = linear_regressor.predict(test_features)\n",
    "linear_train_predictions = linear_regressor.predict(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regressor = RandomForestRegressor(n_estimators=200, random_state=0)\n",
    "rf_regressor.fit(train_features, train_labels)\n",
    "rf_test_predictions = rf_regressor.predict(test_features)\n",
    "rf_train_predictions = rf_regressor.predict(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a GP regression\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "#kernel = 1.0 * Matern(length_scale=1.2, nu=1.5)\n",
    "\n",
    "kernel = 1.0 * Matern(length_scale=1.2, nu=1.5) + WhiteKernel(noise_level=0.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel,n_restarts_optimizer=3)\n",
    "gp.fit(train_features,train_labels)\n",
    "\n",
    "gp_test_predictions, MSE = gp.predict(test_features, return_std=True)\n",
    "gp_train_predictions, MSE_train = gp.predict(train_features, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel='linear')\n",
    "svr.fit(train_features,train_labels)\n",
    "svr_test_predictions = svr.predict(test_features)\n",
    "svr_train_predictions = svr.predict(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost = AdaBoostRegressor(random_state=0, n_estimators=2000)\n",
    "adaboost.fit(train_features,train_labels)\n",
    "adaboost_test_predictions = adaboost.predict(test_features)\n",
    "adaboost_train_predictions = adaboost.predict(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(10, activation='relu', input_shape=[148,input_dimension]),\n",
    "    layers.Dense(10, activation='relu', input_shape=[148,input_dimension]),\n",
    "    layers.Dense(10, activation='relu', input_shape=[148,input_dimension]),\n",
    "    layers.Dense(10, activation='relu', input_shape=[148,input_dimension]),\n",
    "    layers.Dense(10, activation='relu', input_shape=[148,input_dimension]),\n",
    "    layers.Dense(10, activation='relu', input_shape=[148,input_dimension]),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "  model.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "#create the NN\n",
    "model = build_model()\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "history = model.fit(train_features, train_labels,epochs=EPOCHS, validation_split = 0.3, verbose=0,callbacks=[tfdocs.modeling.EpochDots()])\n",
    "\n",
    "nn_test_predictions = model.predict(test_features).flatten()\n",
    "nn_train_predictions = model.predict(train_features).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the metrics for the RF - TRAIN RMSE\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.title(label=\"Random Forest Regression\")\n",
    "plt.scatter(train_labels, rf_train_predictions,label='Train_data',color='orange')\n",
    "\n",
    "plt.scatter(test_labels, rf_test_predictions,label='Test_data',color='dodgerblue')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "lims = [1e-04,1.2]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)\n",
    "plt.legend()\n",
    "\n",
    "print('Random Forest')\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(test_labels, rf_test_predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(test_labels, rf_test_predictions))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(test_labels, rf_test_predictions)))\n",
    "\n",
    "print('Random Forest')\n",
    "#print('Mean Absolute Error:', metrics.mean_absolute_error(test_labels, rf_test_predictions))\n",
    "#print('Mean Squared Error:', metrics.mean_squared_error(test_labels, rf_test_predictions))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(train_labels, rf_train_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(train_labels, linear_train_predictions,label='Train_data',color='orange')\n",
    "plt.title(label=\"Linear Model evaluation\")\n",
    "\n",
    "plt.scatter(test_labels, linear_test_predictions,label='Test_data',color='dodgerblue')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "lims = [1e-04,1.05]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.rc('xtick', labelsize=11)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=11)\n",
    "plt.legend()\n",
    "_ = plt.plot(lims, lims)\n",
    "#plt.savefig(\"parity-plot.png\", dpi=300)\n",
    "\n",
    "print('Linear Model')\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(test_labels, linear_test_predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(test_labels, linear_test_predictions))\n",
    "print('Root Mean Squared Error TEST_data:', np.sqrt(metrics.mean_squared_error(test_labels, linear_test_predictions)))\n",
    "print('Root Mean Squared Error TRAIN_data:', np.sqrt(metrics.mean_squared_error(train_labels, linear_train_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(train_labels, gp_train_predictions,label='Train_data',color='orange')\n",
    "plt.title(label=\"Gaussian Process evaluation\")\n",
    "\n",
    "plt.scatter(test_labels, gp_test_predictions,label='Test_data',color='dodgerblue')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "lims = [1e-04,1.05]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.rc('xtick', labelsize=11)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=11)\n",
    "plt.legend()\n",
    "_ = plt.plot(lims, lims)\n",
    "plt.savefig(\"parity-plot.png\", dpi=300)\n",
    "\n",
    "print('Gaussian Process')\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(test_labels, gp_test_predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(test_labels, gp_test_predictions))\n",
    "print('Root Mean Squared Error TEST_data:', np.sqrt(metrics.mean_squared_error(test_labels, gp_test_predictions)))\n",
    "print('Root Mean Squared Error TRAIN_data:', np.sqrt(metrics.mean_squared_error(train_labels, gp_train_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(train_labels, svr_train_predictions,label='Train_data',color='orange')\n",
    "plt.title(label=\"SVR evaluation\")\n",
    "\n",
    "plt.scatter(test_labels, svr_test_predictions,label='Test_data',color='dodgerblue')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "lims = [1e-04,1.2]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.legend()\n",
    "_ = plt.plot(lims, lims)\n",
    "\n",
    "\n",
    "print('Gaussian Process')\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(test_labels, svr_test_predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(test_labels, svr_test_predictions))\n",
    "print('Root Mean Squared Error TEST_data:', np.sqrt(metrics.mean_squared_error(test_labels, svr_test_predictions)))\n",
    "print('Root Mean Squared Error TRAIN_data:', np.sqrt(metrics.mean_squared_error(train_labels, svr_train_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(train_labels, adaboost_train_predictions,label='Train_data',color='orange')\n",
    "plt.title(label=\"ADAboost evaluation\")\n",
    "\n",
    "plt.scatter(test_labels, adaboost_test_predictions,label='Test_data',color='dodgerblue')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "lims = [1e-04,1.2]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.legend()\n",
    "_ = plt.plot(lims, lims)\n",
    "\n",
    "\n",
    "print('adaboost')\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(test_labels, adaboost_test_predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(test_labels, adaboost_test_predictions))\n",
    "print('Root Mean Squared Error TEST_data:', np.sqrt(metrics.mean_squared_error(test_labels, adaboost_test_predictions)))\n",
    "print('Root Mean Squared Error TRAIN_data:', np.sqrt(metrics.mean_squared_error(train_labels, adaboost_train_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the metrics for the NN\n",
    "\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(train_labels, nn_train_predictions,label='Train_data',color='orange')\n",
    "\n",
    "plt.title(label=\"Neural Net evaluation\")\n",
    "plt.scatter(test_labels, nn_test_predictions,label='Test_data',color='dodgerblue')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "lims = [1e-04,1.2]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.legend()\n",
    "_ = plt.plot(lims, lims)\n",
    "\n",
    "\n",
    "print('Neural Net')\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(test_labels, nn_test_predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(test_labels, nn_test_predictions))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(test_labels, nn_test_predictions)))\n",
    "\n",
    "print('Train Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(train_labels, nn_train_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess feature importance for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest for feature importance on a regression problem\n",
    "'''\n",
    "rf_regressor = RandomForestRegressor(n_estimators=200, random_state=0)\n",
    "rf_regressor.fit(train_features, train_labels)\n",
    "rf_test_predictions = rf_regressor.predict(test_features)\n",
    "rf_train_predictions = rf_regressor.predict(train_features)\n",
    "\n",
    "# get importance\n",
    "importance = rf_regressor.feature_importances_\n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.figure(figsize=(10, 4))\n",
    "\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "\n",
    "pyplot.title(label=\"Feature Importance Evaluation\")\n",
    "pyplot.ylabel('Importance')\n",
    "pyplot.xlabel('Features')\n",
    "\n",
    "pyplot.show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
